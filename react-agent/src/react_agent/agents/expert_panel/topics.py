"""Expert Panel Topics - ì‹ ê·œ í† í”½ ìˆ˜ì§‘ ë° ê´€ë¦¬

ë§¤ì£¼ ìˆ˜ì§‘ë˜ëŠ” ì •ë³´ì—ì„œ ì‹ ê·œ í† í”½ì„ ì¶”ì¶œí•˜ê³  ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ
"""

import os
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


# í† í”½ ë¶„ë¥˜ í‚¤ì›Œë“œ ë§¤í•‘
TOPIC_CATEGORIES = {
    "ì •ì±…/ë²•ê·œ": [
        "ë²•ë¥ ", "ë²•", "ê·œì œ", "ì •ì±…", "ê¸°ë³¸ê³„íš", "ì‹œí–‰ë ¹", "ì§€ì¹¨",
        "NDC", "íƒ„ì†Œì¤‘ë¦½", "ë„·ì œë¡œ", "CBAM", "COP", "í˜‘ì•½"
    ],
    "íƒ„ì†Œë°°ì¶œê¶Œ": [
        "KAU", "KCU", "KOC", "ë°°ì¶œê¶Œ", "í• ë‹¹", "ìƒì‡„", "ì™¸ë¶€ì‚¬ì—…",
        "í¬ë ˆë”§", "ê±°ë˜ì œ", "VCS", "CDM", "ITMO"
    ],
    "ì‹œì¥/ê±°ë˜": [
        "ê°€ê²©", "ì‹œì„¸", "ê±°ë˜", "ì‹œì¥", "ETS", "ê²½ë§¤", "ìœ ë™ì„±",
        "ì„ ë¬¼", "íˆ¬ì", "ì „ë§", "ì˜ˆì¸¡"
    ],
    "ê°ì¶•ê¸°ìˆ ": [
        "CCUS", "CCS", "ìˆ˜ì†Œ", "ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥",
        "ESS", "DAC", "ê¸°ìˆ ", "íš¨ìœ¨", "ì „ê¸°í™”", "SMR"
    ],
    "MRV/ê²€ì¦": [
        "Scope", "ë°°ì¶œëŸ‰", "ì‚°ì •", "ê²€ì¦", "ëª¨ë‹ˆí„°ë§", "ë³´ê³ ",
        "GHG Protocol", "ISO 14064", "ì¸ë²¤í† ë¦¬", "íƒ„ì†Œë°œìêµ­"
    ]
}


def get_knowledge_base_path() -> Path:
    """ì§€ì‹ë² ì´ìŠ¤ ê²½ë¡œ ë°˜í™˜"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    kb_path = os.getenv("KNOWLEDGE_BASE_PATH", "knowledge_base")
    return Path(kb_path)


def get_recent_documents(days: int = 7) -> List[Dict[str, Any]]:
    """ìµœê·¼ Nì¼ ë‚´ ì¶”ê°€ëœ ë¬¸ì„œ ëª©ë¡ ë°˜í™˜

    Args:
        days: ì¡°íšŒ ê¸°ê°„ (ì¼)

    Returns:
        ìµœê·¼ ì¶”ê°€ëœ ë¬¸ì„œ ëª©ë¡
    """
    kb_path = get_knowledge_base_path()

    if not kb_path.exists():
        logger.warning(f"ì§€ì‹ë² ì´ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {kb_path}")
        return []

    recent_docs = []
    cutoff_date = datetime.now() - timedelta(days=days)

    try:
        for file_path in kb_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in [".pdf", ".txt", ".md", ".json"]:
                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

                if mtime >= cutoff_date:
                    # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
                    title = file_path.stem.replace("_", " ").replace("-", " ")

                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    category = _classify_topic(title + " " + str(file_path))

                    recent_docs.append({
                        "title": title,
                        "path": str(file_path),
                        "date_added": mtime.strftime("%Y-%m-%d"),
                        "category": category,
                        "file_type": file_path.suffix.lower()
                    })

        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        recent_docs.sort(key=lambda x: x["date_added"], reverse=True)

    except Exception as e:
        logger.error(f"ìµœê·¼ ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")

    return recent_docs


def _classify_topic(text: str) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ í† í”½ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
    text_lower = text.lower()

    # ê° ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    scores = {}
    for category, keywords in TOPIC_CATEGORIES.items():
        score = sum(1 for kw in keywords if kw.lower() in text_lower)
        if score > 0:
            scores[category] = score

    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
    if scores:
        best_category = max(scores.keys(), key=lambda k: scores.get(k, 0))
        return best_category

    return "ì¼ë°˜"


def extract_weekly_updates() -> List[Dict[str, Any]]:
    """ì´ë²ˆ ì£¼ ì£¼ìš” ì—…ë°ì´íŠ¸ ì¶”ì¶œ

    Returns:
        ì£¼ê°„ ì—…ë°ì´íŠ¸ ëª©ë¡
    """
    # ìµœê·¼ 7ì¼ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    recent_docs = get_recent_documents(days=7)

    updates = []
    for doc in recent_docs[:10]:  # ìµœëŒ€ 10ê°œ
        updates.append({
            "title": doc["title"],
            "date": doc["date_added"],
            "category": doc["category"],
            "summary": f"{doc['category']} ë¶„ì•¼ ì‹ ê·œ ìë£Œ",
            "source": doc.get("path", "")
        })

    return updates


def get_topics_by_category(category: str, days: int = 30) -> List[Dict[str, Any]]:
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìµœê·¼ í† í”½ ë°˜í™˜

    Args:
        category: í† í”½ ì¹´í…Œê³ ë¦¬
        days: ì¡°íšŒ ê¸°ê°„ (ì¼)

    Returns:
        í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í† í”½ ëª©ë¡
    """
    recent_docs = get_recent_documents(days=days)

    return [doc for doc in recent_docs if doc.get("category") == category]


def get_trending_topics(days: int = 30) -> Dict[str, int]:
    """íŠ¸ë Œë”© í† í”½ ë¶„ì„ (ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜)

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)

    Returns:
        ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜
    """
    recent_docs = get_recent_documents(days=days)

    # ì¹´í…Œê³ ë¦¬ë³„ ì¹´ìš´íŠ¸
    category_counts = {}
    for doc in recent_docs:
        category = doc.get("category", "ì¼ë°˜")
        category_counts[category] = category_counts.get(category, 0) + 1

    # ì •ë ¬í•˜ì—¬ ë°˜í™˜
    return dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True))


def format_weekly_summary() -> str:
    """ì£¼ê°„ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
    updates = extract_weekly_updates()
    trending = get_trending_topics(days=7)

    if not updates and not trending:
        return "ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."

    summary_parts = []

    # íŠ¸ë Œë”© í† í”½
    if trending:
        summary_parts.append("ğŸ“Š **ì´ë²ˆ ì£¼ ì£¼ìš” ë¶„ì•¼**")
        for category, count in list(trending.items())[:5]:
            summary_parts.append(f"  - {category}: {count}ê±´")

    # ì‹ ê·œ ë¬¸ì„œ
    if updates:
        summary_parts.append("\nğŸ“„ **ì‹ ê·œ ìë£Œ**")
        for update in updates[:5]:
            summary_parts.append(f"  - [{update['category']}] {update['title']} ({update['date']})")

    return "\n".join(summary_parts)


# ============ ì „ë¬¸ê°€ë³„ í† í”½ ë§¤í•‘ ============

EXPERT_TOPIC_MAPPING = {
    "policy_expert": "ì •ì±…/ë²•ê·œ",
    "carbon_credit_expert": "íƒ„ì†Œë°°ì¶œê¶Œ",
    "market_expert": "ì‹œì¥/ê±°ë˜",
    "technology_expert": "ê°ì¶•ê¸°ìˆ ",
    "mrv_expert": "MRV/ê²€ì¦"
}


def get_expert_recent_topics(expert_role: str, days: int = 14) -> List[Dict[str, Any]]:
    """íŠ¹ì • ì „ë¬¸ê°€ ì˜ì—­ì˜ ìµœê·¼ í† í”½ ë°˜í™˜

    Args:
        expert_role: ì „ë¬¸ê°€ ì—­í•  (ì˜ˆ: "policy_expert")
        days: ì¡°íšŒ ê¸°ê°„ (ì¼)

    Returns:
        í•´ë‹¹ ì „ë¬¸ê°€ ì˜ì—­ì˜ ìµœê·¼ í† í”½ ëª©ë¡
    """
    category = EXPERT_TOPIC_MAPPING.get(expert_role)
    if not category:
        return []

    return get_topics_by_category(category, days=days)


def get_expert_topic_summary(expert_role: str) -> str:
    """ì „ë¬¸ê°€ë³„ í† í”½ ìš”ì•½ ìƒì„±

    Args:
        expert_role: ì „ë¬¸ê°€ ì—­í• 

    Returns:
        í•´ë‹¹ ì „ë¬¸ê°€ ì˜ì—­ì˜ í† í”½ ìš”ì•½ í…ìŠ¤íŠ¸
    """
    topics = get_expert_recent_topics(expert_role, days=14)

    if not topics:
        category = EXPERT_TOPIC_MAPPING.get(expert_role, expert_role)
        return f"ìµœê·¼ 2ì£¼ê°„ {category} ë¶„ì•¼ì˜ ì‹ ê·œ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."

    summary_parts = [f"ğŸ“š **ìµœê·¼ 2ì£¼ê°„ ì‹ ê·œ ìë£Œ** (ì´ {len(topics)}ê±´)"]

    for topic in topics[:5]:
        summary_parts.append(f"  ğŸ“„ {topic['title']} ({topic['date_added']})")

    if len(topics) > 5:
        summary_parts.append(f"  ... ì™¸ {len(topics) - 5}ê±´")

    return "\n".join(summary_parts)


# ============ ìºì‹œ ê´€ë¦¬ (ì„ íƒì ) ============

_topic_cache: Dict[str, Any] = {}
_cache_time: Optional[datetime] = None
CACHE_DURATION = timedelta(hours=1)  # 1ì‹œê°„ ìºì‹œ


def get_cached_topics() -> Optional[Dict[str, Any]]:
    """ìºì‹œëœ í† í”½ ì •ë³´ ë°˜í™˜"""
    global _topic_cache, _cache_time

    if _cache_time and datetime.now() - _cache_time < CACHE_DURATION:
        return _topic_cache

    return None


def update_topic_cache():
    """í† í”½ ìºì‹œ ì—…ë°ì´íŠ¸"""
    global _topic_cache, _cache_time

    _topic_cache = {
        "weekly_updates": extract_weekly_updates(),
        "trending": get_trending_topics(days=7),
        "recent_docs": get_recent_documents(days=7)
    }
    _cache_time = datetime.now()

    logger.info(f"í† í”½ ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(_topic_cache.get('recent_docs', []))}ê°œ ë¬¸ì„œ")

    return _topic_cache


def get_all_topics_info() -> Dict[str, Any]:
    """ì „ì²´ í† í”½ ì •ë³´ ë°˜í™˜ (ìºì‹œ í™œìš©)"""
    cached = get_cached_topics()
    if cached:
        return cached

    return update_topic_cache()
